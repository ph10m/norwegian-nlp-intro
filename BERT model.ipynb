{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2be8efae-32cc-4fce-8ab5-9ba8f1d22b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d76f23a7-c8dc-49c9-95c7-51b6d54a1a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_getter(name):\n",
    "    return os.path.join(os.getcwd(), 'data', name + '.json')\n",
    "data = {}\n",
    "files = ['train', 'test', 'dev']\n",
    "for json_data in files:\n",
    "    with open(file_getter(json_data)) as infile:\n",
    "        data[json_data] = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c34645b-fd64-4ca6-b08d-b39b20c96252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_map(xs):\n",
    "    ys = []\n",
    "    for x in xs:\n",
    "        ys.extend(x)\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a0ac04-bfa4-48b6-b015-16a19916cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'Polarity': sentiment label ('Negative', 'Positive')\n",
    "#'Intensity': sentiment intensity ('Standard', 'Strong', 'Slight')\n",
    "polarities = {\n",
    "    'Negative': -1,\n",
    "    'Positive': 1\n",
    "}\n",
    "intensities = {\n",
    "    'Standard': 1,\n",
    "    'Slight': 2,\n",
    "    'Strong': 3\n",
    "}\n",
    "\n",
    "def make_valence(v):\n",
    "    if v == 0:\n",
    "        return 1\n",
    "    elif v > 1:\n",
    "        return 2\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def make_valence(v):\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ca9758f-0672-494a-a9ad-c09f7536bdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pattern = re.compile('[\\W_]+')\n",
    "clean = lambda text : pattern.sub(' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1536dbe9-0f7a-456f-b8cc-b350d518f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sent = -3\n",
    "max_sent = 3\n",
    "\n",
    "def norm_sent(sent_arr):\n",
    "    if len(sent_arr) == 0:\n",
    "        # average 0 1 _2_ 3 4\n",
    "        return 2\n",
    "    avg = sum(sent_arr)/len(sent_arr)\n",
    "    normed = (avg-min_sent) / (max_sent - min_sent)\n",
    "    # beef the sentiments up to 4 for more sparsity (0-4) => mimicking 1-5 stars\n",
    "    return round(normed * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8619338d-eac1-4d6b-8097-eae87bee64a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiments():\n",
    "    texts = []\n",
    "    sentiments = []\n",
    "    for file in files:\n",
    "        tmp = data[file]\n",
    "        for item in tmp:\n",
    "            text_valence = []\n",
    "            opinions = item['opinions']\n",
    "            if len(opinions) > 0:\n",
    "                # handle sentiment\n",
    "                targets = []\n",
    "                for opinion in opinions:\n",
    "                    target, target_span = opinion['Target']\n",
    "                    polarity = opinion['Polarity']\n",
    "                    intensity = opinion['Intensity']\n",
    "\n",
    "                    if len(target) > 0 and polarity is not None and intensity is not None:\n",
    "                        #targets.append([target[0], target_span[0], valence])\n",
    "                        text_valence.append(polarities[polarity] * intensities[intensity])\n",
    "                #unique_targets = [list(x) for x in set(tuple(x) for x in targets)]            \n",
    "                cleaned = clean(item['text']).strip()\n",
    "                MIN_SENT_LEN = 50\n",
    "                if len(cleaned) > MIN_SENT_LEN:\n",
    "                    texts.append(cleaned)\n",
    "                    sentiments.append(norm_sent(text_valence))\n",
    "\n",
    "                    \n",
    "    return [texts, sentiments]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b550575f-1626-445c-a4ca-06d4eab2c4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baksiden er sort blank og skinnende med et dek...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Skjermen er helt klart visuelt utfordrende noe...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Uansett hva man måtte mene om utseendet så er ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>En mer allsidig og tilkoblingsvennlig skjerm h...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skjermen kan bare vippes ikke heves senkes ell...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4735</th>\n",
       "      <td>På sitt sterkeste og beste er stykket nettopp ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4736</th>\n",
       "      <td>Dette igjen stiller store krav til diksjon og ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4737</th>\n",
       "      <td>Især Thomas Bipin Olsen sliter til tider men e...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4738</th>\n",
       "      <td>Også Iselin Shumba med sin sceniske sikkerhet ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4739</th>\n",
       "      <td>Sistnevnte viser seg også som en fin komiker i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4740 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  sentiment\n",
       "0     Baksiden er sort blank og skinnende med et dek...          3\n",
       "1     Skjermen er helt klart visuelt utfordrende noe...          3\n",
       "2     Uansett hva man måtte mene om utseendet så er ...          3\n",
       "3     En mer allsidig og tilkoblingsvennlig skjerm h...          2\n",
       "4     Skjermen kan bare vippes ikke heves senkes ell...          1\n",
       "...                                                 ...        ...\n",
       "4735  På sitt sterkeste og beste er stykket nettopp ...          3\n",
       "4736  Dette igjen stiller store krav til diksjon og ...          2\n",
       "4737  Især Thomas Bipin Olsen sliter til tider men e...          2\n",
       "4738  Også Iselin Shumba med sin sceniske sikkerhet ...          3\n",
       "4739  Sistnevnte viser seg også som en fin komiker i...          2\n",
       "\n",
       "[4740 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts, sentiments = get_sentiments()\n",
    "df = pd.DataFrame(list(zip(texts, sentiments)), columns = ['text', 'sentiment'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b549ede-22f0-4471-a7fa-4cfe4a114337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.sentiment.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c45487f-f043-4869-b19a-f3c5d6e8e42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%df.to_csv('sentiment_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "898534ab-9648-4727-afbf-f33988fcc4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4740.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.403797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.010927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment\n",
       "count  4740.000000\n",
       "mean      2.403797\n",
       "std       1.010927\n",
       "min       0.000000\n",
       "25%       2.000000\n",
       "50%       3.000000\n",
       "75%       3.000000\n",
       "max       4.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbb08645-fdd1-499d-be99-f3072f35a7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 1, 4, 0], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af6d6c00-5622-44d3-9d71-02420f706499",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ltgoslo/norbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "690c3a13-e6a3-4094-8609-1bb62f828149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 64\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "# EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bcc731b1-4d88-4a9e-90cc-9c2d20ef7f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentData(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = self.data.sentiment\n",
    "        #self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            #max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f617be0e-161f-4670-90b7-6a27f956f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "330130cc-7985-414f-b27b-11ae0e8c47c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (4740, 2)\n",
      "TRAIN Dataset: (3792, 2)\n",
      "TEST Dataset: (948, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train.shape))\n",
    "print(\"TEST Dataset: {}\".format(test.shape))\n",
    "\n",
    "training_set = SentimentData(train, tokenizer, MAX_LEN)\n",
    "testing_set = SentimentData(test, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "60fd555b-b729-4ac9-81a4-b851c865c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 1,\n",
    "                'pin_memory': True\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 1,\n",
    "                'pin_memory': True\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ac76779c-1839-407a-8db4-1515c3cf88cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norec(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(Norec, self).__init__()\n",
    "        self.l1 = model\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 5)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "06380012-1d77-470f-99f8-c9608b3a5786",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Norec(AutoModel.from_pretrained(model_name, num_labels=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0f5ce043-cb53-4d11-a020-262095a1648d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6a80bfed-4eb9-4a55-be0c-dd9fd2229b54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9f8b8fce-50bb-49d3-bc4e-baa7235de3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0360b7ee-17a1-4b56-8bbf-3844bf512476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcuate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07252b71-5834-48be-a2fb-c866b3de964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(training_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9f3683cb-5306-4d3c-b2b5-0020cd81ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
    "\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accuracy(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7480d29-bbc1-48ae-a1dc-e7049d8b0e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073bd950-5606-4bb9-9138-593c0e0a709b",
   "metadata": {},
   "source": [
    "# BERT (NorBert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1e406e2-6ae7-41d9-93cd-b3adacc7fef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, pipeline, Trainer, TrainingArguments, AdamW\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9f0d55a3-9c85-4b28-aae0-b97040fbe06b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33552/798972679.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNorecFineDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_encodings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNorecFineDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_encodings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_labels' is not defined"
     ]
    }
   ],
   "source": [
    "class NorecFineDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "train_dataset = NorecFineDataset(train_encodings, train_labels)\n",
    "val_dataset = NorecFineDataset(val_encodings, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e213bc9-819d-4466-ab64-9febd0b89c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6e4e69f3-7258-4213-ae8f-a57922ab96ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1523"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class NorecFine(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "# X_train, X_val, y_train, y_val\n",
    "\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=True)\n",
    "\n",
    "train_data = NorecFine(train_encodings, y_train)\n",
    "val_data = NorecFine(val_encodings, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f65faddd-06fa-4d14-97ee-37edd29f946a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [102, 11864, 3638, 1238, 20489, 3605, 103], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"dette er rævva\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a95d0e39-b03e-4413-b64e-6eb9a11e8bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(X_train, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a19baa24-e733-482f-a524-518ceef44bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 3\n",
    "model = AutoModel.from_pretrained(model_name, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e3bd4cc0-e951-4fb5-80d0-3e41c3cc77ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"test sent\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ee0d362f-1ab7-4a60-9f25-9d069ece20f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(output_dir=test sent, overwrite_output_dir=False, do_train=False, do_eval=None, do_predict=False, evaluation_strategy=IntervalStrategy.EPOCH, prediction_loss_only=False, per_device_train_batch_size=10, per_device_eval_batch_size=10, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.01, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs\\May25_18-24-31_desktop, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=test sent, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model=accuracy, greater_is_better=True, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1, mp_parameters=)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "49cb6c7b-8931-44f1-9813-e32be37baecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f2a39bec-9178-4995-a903-404ccb9ebc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_encodings,\n",
    "    eval_dataset=val_encodings,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4df25d4a-aedc-4582-ba6b-46a4fbd46775",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33552/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\tollef\\documents\\git\\nlp-intro\\venv\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[0;32m   1099\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1101\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m                 \u001b[1;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tollef\\documents\\git\\nlp-intro\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tollef\\documents\\git\\nlp-intro\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tollef\\documents\\git\\nlp-intro\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\tollef\\documents\\git\\nlp-intro\\venv\\lib\\site-packages\\transformers\\data\\data_collator.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         batch = self.tokenizer.pad(\n\u001b[0m\u001b[0;32m    119\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tollef\\documents\\git\\nlp-intro\\venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mpad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   2563\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_input_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mencoded_inputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2564\u001b[0m             raise ValueError(\n\u001b[1;32m-> 2565\u001b[1;33m                 \u001b[1;34m\"You should supply an encoding or a list of encodings to this method\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2566\u001b[0m                 \u001b[1;34mf\"that includes {self.model_input_names[0]}, but you provided {list(encoded_inputs.keys())}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2567\u001b[0m             )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd5362e-6143-40f4-b5e6-f707c7086db9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
