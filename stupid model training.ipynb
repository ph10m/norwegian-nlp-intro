{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31401510-a44a-417b-8b89-35e3a283fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb1c5569-80ad-49a3-813b-d7671645bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9379adad-a30e-41d6-a389-0d14d13b2fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434cd9e9-7625-4901-8d7a-0e01dab22df2",
   "metadata": {},
   "source": [
    "## Handle the CV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cccf32d4-7af4-4d5b-97af-7aad7339730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_path = os.path.join(os.getcwd(), 'data', 'sharepoint', 'cv')\n",
    "cvs = os.listdir(cv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b818311c-50fd-48a2-8b0a-c3ebaf57f35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv(i):\n",
    "    return os.path.join(cv_path, cvs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec9c8728-3466-4de5-8a19-56f410556020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\tollef\\\\Documents\\\\Git\\\\nlp-intro\\\\data\\\\sharepoint\\\\cv\\\\albert danielsen.txt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = get_cv(0)\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a0c64d-752f-4179-86f1-3de184384e1e",
   "metadata": {},
   "source": [
    "## Fetch and cleandata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c2d11e0-af30-4f9d-9b91-d05c6f4239eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x1c75a534600>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare spacy nlp object\n",
    "spacy_model = 'nb_core_news_lg'\n",
    "nlp = spacy.load(spacy_model, disable = ['parser', 'tagger', 'ner'])\n",
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daeab1a3-9a9d-4a73-9489-10bc1226cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "with open(example, 'r', encoding='utf-8') as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8fc6bc1-93cd-4057-8b8e-1c7366595af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(text):\n",
    "    skip_pattern = '\\r\\n \\n\\n \\n\\n\\n!\"-#$%&()--.*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r '\n",
    "    return [token for token in text if token not in skip_pattern]\n",
    "\n",
    "def clean_sent(sent):\n",
    "    illegal = ['\\n', '\\t', '.', ':']\n",
    "    def valid(tok):\n",
    "        return (tok not in illegal) and tok.isalpha()\n",
    "    \n",
    "    return ' '.join([tok for tok in sent.split(' ') if valid(tok)])\n",
    "    \n",
    "\n",
    "# combine all data, use the spacy sentencizer\n",
    "def get_sentences(data):\n",
    "    sentences = [clean_sent(sent).strip() for sent in data]\n",
    "    return '. '.join(sentences).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cdcbbfe-e436-4d13-a92a-63e46f572b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albert er en svilingeniør innen kybernetikk og utdannet ved med spesialisering innen embedded Han har opparbeidet seg bred erfaring innen programvareutvikling fra sine prosjekter hos tidligere arbeidsgivere og som. . Nylig har Albert jobbet i et prosjekt for Altera et stort med mål om å digitalisere arbeidsflyten ved modifikasjoner på skip i Hans bidrag i dette prosjektet består i utvikling av en responsiv og brukervennlig løsning i samt utvikling i Core i et domain driven Gjennomførelsen av prosjektet ble gjort ved bruk av Azure DevOps og agile. . Tidligere har Albert bidratt på et stort prosjekt for titulert APS av Personell tok sikte på å automatisere registrering og utdeling av sertifikater til sjøfolk som opererer i norske Løsningen ble utviklet med Azure Functions og message med en. . Alberts kompetanse inkluderer programmering av mikrokontrollere og design av hardware og kompetanse han har styrket ved praktisk gjennomføring av prosjekt og deltidsjobb for ved å utvikle hardware og software for et maskinelt kalibreringsverkøy for fjærstivhet og en modulær robotisk Han også erfaring med platformer og gjennom sommerjobb hos Mer spesifikt har han jobbet med Microsoft Samsung HMD Odyssey og. . Teknologier og verktøy Albert behersker og liker å arbeide med er Azure Power Autodesk og Han holder sertifiseringer Azure og. . Albert er en konsulent med lidenskap for digitaliseringsoppgaver og tar imot alle oppgaver med godt Han finner mye glede i å samarbeide med jobber alltid strukturert og brenner for å levere gode. . . . Page. Prosjektreferanser. . Infrastructure. . Klient En av de største skipsrederne i ModifikasjonsdatabaseRolle og Full stack utviklerOvertakelse av et eksisterende system og utviklingen av en ny løsning for gjennomføring av modifikasjoner på marine fartøy i Systemet tillater brukere å få innsyn i og følge opp viktige beslutninger som del av modifikasjoner på og Agile Azure Cypres. . . . Petrojarl AS. . Klient En av de største skipseierne i. . Intern webside. . Rolle og Full stack utvikler. . Ansvar inkluderer front og back end utvikling av en webapp kalt project Løsningen muliggjør oppfølging av prosjekter i og legger til rette for bedre informasjonsdeling på tvers av. . Metode og DevOps. . . . . . Klient Statlig etat med ansvar for at og sikkerhetsstandarder for skip i norske farvann blir møtt. . Automatisk personellsertifikat. . Rolle og Full stack utvikler. . Ansvar inkluderer front end og back end utvikling av et system for automatisk utstedelse av sertifikater til skipspersonell gjennom en web. . Metode og DevOps. . Sealab. . Klient Et av europas største Avdelingen for og. . Dexterous. . Rolle og. . Arbeidet foregikk i SINTEF prosjektet iProcess med å lage en finmotorisk robothånd for håndtering av deformerbare råvarer som fisk og. . En robothånd med finmotoriske. . Metode og. . . . Handel. . Klient Kjøpesenter i. . Booking av ved Sirkus. . Rolle og. . Ansvar for å lage modeller i backend i Django og nettsidefunksjonalitet og utseende i Hadde fokus på å oppfylle kundens ønsker og brukte for å gjennomføre. . Webapplikasjon for booking av. . Metode og. . . . Ocean. . Klient Et av europas største Avdelingen for marinteknisk. . . . Rolle og Teknisk. . Teknisk utførende i prosjektet ut ifra avtalt teknisk med oppgave å programmere en testrigg for avstiving og verifikasjon av stivhet i. . En automatisk og tidsbesparende rigg for testing av Kunden fikk oppfylt sine krav og riggen brukes i dag i. . Metode og. . . . . . Klient Norsk og Blant verdens største selskaper i. . Digitaliseringsinitiativer i drift. . Rolle og Summer. . Blant initiativene var digital bruk av nettbrett og HoloLens i digital software modeller av fysiske anlegg og visualisering av live datafremstilling fra anlegg med Power Brukte ingeniørfaglige metoder og lærte prosjektarbeid gjennom samarbeid med operasjonsenhetene i. . Interaktiv brukerguide for live Power BI dashboard og digitalt Dashboardet brukes i dag i vedlikeholdsavdelingen for å tilgjengeliggjøre mens brukerguiden brukes for å lære opp personell i bruk av HoloLens med digitale. . Metode og Microsoft Office Power. . . . . . Utdannelse. . Kybernetikk og robotikk Tilpassede. . NTNU. . Elektroingeniør. . NTNU. . . . Arbeidserfaring. . . . Ocean. . . . AS. . . . Kurs. . Engineering Design Principles. . Redux Router Kurs. . . . Sertifiseringer. . MCSA Web Applications. . Programming in with JavaScript and. . Programming in. . Developing MVC Web Applications. . Microsoft Azure Fundamentals. . Microsoft Certified Azure Administrator Associate\n"
     ]
    }
   ],
   "source": [
    "text = get_sentences(data)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1482ea6d-6f89-4db0-81ae-e065012b1489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Albert er en svilingeniør innen kybernetikk og utdannet ved med spesialisering innen embedded Han har opparbeidet seg bred erfaring innen programvareutvikling fra sine prosjekter hos tidligere arbeidsgivere og som',\n",
       " 'Nylig har Albert jobbet i et prosjekt for Altera et stort med mål om å digitalisere arbeidsflyten ved modifikasjoner på skip i Hans bidrag i dette prosjektet består i utvikling av en responsiv og brukervennlig løsning i samt utvikling i Core i et domain driven Gjennomførelsen av prosjektet ble gjort ved bruk av Azure DevOps og agile',\n",
       " 'Tidligere har Albert bidratt på et stort prosjekt for titulert APS av Personell tok sikte på å automatisere registrering og utdeling av sertifikater til sjøfolk som opererer i norske Løsningen ble utviklet med Azure Functions og message med en',\n",
       " 'Alberts kompetanse inkluderer programmering av mikrokontrollere og design av hardware og kompetanse han har styrket ved praktisk gjennomføring av prosjekt og deltidsjobb for ved å utvikle hardware og software for et maskinelt kalibreringsverkøy for fjærstivhet og en modulær robotisk Han også erfaring med platformer og gjennom sommerjobb hos Mer spesifikt har han jobbet med Microsoft Samsung HMD Odyssey og',\n",
       " 'Albert er en konsulent med lidenskap for digitaliseringsoppgaver og tar imot alle oppgaver med godt Han finner mye glede i å samarbeide med jobber alltid strukturert og brenner for å levere gode',\n",
       " 'Klient En av de største skipsrederne i ModifikasjonsdatabaseRolle og Full stack utviklerOvertakelse av et eksisterende system og utviklingen av en ny løsning for gjennomføring av modifikasjoner på marine fartøy i Systemet tillater brukere å få innsyn i og følge opp viktige beslutninger som del av modifikasjoner på og Agile Azure Cypres',\n",
       " 'Ansvar inkluderer front og back end utvikling av en webapp kalt project Løsningen muliggjør oppfølging av prosjekter i og legger til rette for bedre informasjonsdeling på tvers av',\n",
       " 'Ansvar for å lage modeller i backend i Django og nettsidefunksjonalitet og utseende i Hadde fokus på å oppfylle kundens ønsker og brukte for å gjennomføre',\n",
       " 'Blant initiativene var digital bruk av nettbrett og HoloLens i digital software modeller av fysiske anlegg og visualisering av live datafremstilling fra anlegg med Power Brukte ingeniørfaglige metoder og lærte prosjektarbeid gjennom samarbeid med operasjonsenhetene i',\n",
       " 'Interaktiv brukerguide for live Power BI dashboard og digitalt Dashboardet brukes i dag i vedlikeholdsavdelingen for å tilgjengeliggjøre mens brukerguiden brukes for å lære opp personell i bruk av HoloLens med digitale']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_doc = nlp(text)\n",
    "MIN_LEN = 25\n",
    "sentences = [s.text.replace('.','').strip() for s in spacy_doc.sents if len(s) > MIN_LEN]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01837d2-cb76-499b-a2d4-0130a71b1971",
   "metadata": {},
   "source": [
    "## Process the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13b2677b-133a-4687-94a6-254d97829edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b2d0199-cca0-48f4-9924-8456f0006b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 4562 characters\n",
      "Albert er en svilingeniør innen kybernetikk og utdannet ved med spesialisering innen embedded Han har opparbeidet seg bred erfaring innen programvareutvikling fra sine prosjekter hos tidligere arbeids...\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of text: {len(text)} characters')\n",
    "print(text[:200] + '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68601cfa-00eb-40ba-991e-484af966aae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bb85d43-6777-4109-a34e-b14fac70bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = tf.strings.unicode_split(sentences, input_encoding='UTF-8')\n",
    "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
    "ids = ids_from_chars(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e628b52-2f21-4d9e-be7b-c6eadb0c3083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'Albert er en svilingeni\\xc3\\xb8r innen kybernetikk og utdannet ved med spesialisering innen embedded Han har opparbeidet seg bred erfaring innen programvareutvikling fra sine prosjekter hos tidligere arbeidsgivere og som',\n",
       "       b'Nylig har Albert jobbet i et prosjekt for Altera et stort med m\\xc3\\xa5l om \\xc3\\xa5 digitalisere arbeidsflyten ved modifikasjoner p\\xc3\\xa5 skip i Hans bidrag i dette prosjektet best\\xc3\\xa5r i utvikling av en responsiv og brukervennlig l\\xc3\\xb8sning i samt utvikling i Core i et domain driven Gjennomf\\xc3\\xb8relsen av prosjektet ble gjort ved bruk av Azure DevOps og agile',\n",
       "       b'Tidligere har Albert bidratt p\\xc3\\xa5 et stort prosjekt for titulert APS av Personell tok sikte p\\xc3\\xa5 \\xc3\\xa5 automatisere registrering og utdeling av sertifikater til sj\\xc3\\xb8folk som opererer i norske L\\xc3\\xb8sningen ble utviklet med Azure Functions og message med en',\n",
       "       b'Alberts kompetanse inkluderer programmering av mikrokontrollere og design av hardware og kompetanse han har styrket ved praktisk gjennomf\\xc3\\xb8ring av prosjekt og deltidsjobb for ved \\xc3\\xa5 utvikle hardware og software for et maskinelt kalibreringsverk\\xc3\\xb8y for fj\\xc3\\xa6rstivhet og en modul\\xc3\\xa6r robotisk Han ogs\\xc3\\xa5 erfaring med platformer og gjennom sommerjobb hos Mer spesifikt har han jobbet med Microsoft Samsung HMD Odyssey og',\n",
       "       b'Albert er en konsulent med lidenskap for digitaliseringsoppgaver og tar imot alle oppgaver med godt Han finner mye glede i \\xc3\\xa5 samarbeide med jobber alltid strukturert og brenner for \\xc3\\xa5 levere gode',\n",
       "       b'Klient En av de st\\xc3\\xb8rste skipsrederne i ModifikasjonsdatabaseRolle og Full stack utviklerOvertakelse av et eksisterende system og utviklingen av en ny l\\xc3\\xb8sning for gjennomf\\xc3\\xb8ring av modifikasjoner p\\xc3\\xa5 marine fart\\xc3\\xb8y i Systemet tillater brukere \\xc3\\xa5 f\\xc3\\xa5 innsyn i og f\\xc3\\xb8lge opp viktige beslutninger som del av modifikasjoner p\\xc3\\xa5 og Agile Azure Cypres',\n",
       "       b'Ansvar inkluderer front og back end utvikling av en webapp kalt project L\\xc3\\xb8sningen muliggj\\xc3\\xb8r oppf\\xc3\\xb8lging av prosjekter i og legger til rette for bedre informasjonsdeling p\\xc3\\xa5 tvers av',\n",
       "       b'Ansvar for \\xc3\\xa5 lage modeller i backend i Django og nettsidefunksjonalitet og utseende i Hadde fokus p\\xc3\\xa5 \\xc3\\xa5 oppfylle kundens \\xc3\\xb8nsker og brukte for \\xc3\\xa5 gjennomf\\xc3\\xb8re',\n",
       "       b'Blant initiativene var digital bruk av nettbrett og HoloLens i digital software modeller av fysiske anlegg og visualisering av live datafremstilling fra anlegg med Power Brukte ingeni\\xc3\\xb8rfaglige metoder og l\\xc3\\xa6rte prosjektarbeid gjennom samarbeid med operasjonsenhetene i',\n",
       "       b'Interaktiv brukerguide for live Power BI dashboard og digitalt Dashboardet brukes i dag i vedlikeholdsavdelingen for \\xc3\\xa5 tilgjengeliggj\\xc3\\xb8re mens brukerguiden brukes for \\xc3\\xa5 l\\xc3\\xa6re opp personell i bruk av HoloLens med digitale'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "chars = chars_from_ids(ids)\n",
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a85969e-95b4-473c-8610-8545c0571dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1618ca04-fe47-4d88-8bc4-8b9552e7d556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4562,), dtype=int64, numpy=array([ 3, 36, 26, ..., 25, 43, 29], dtype=int64)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f254d805-1a4d-47d5-a45f-9341389757f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (sequences.map(split_input_target)\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4113f61-2667-40dd-9692-6d3d3bc67c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "class CvModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "model = CvModel(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18ef009a-843b-42d4-9785-d07337ec5003",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25cb463a-6c57-4225-8060-dc0f6f2fc5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b' sj\\xc3\\xb8folk som opererer i norske L\\xc3\\xb8sningen ble utviklet med Azure Functions og message med en. . Alber'\n",
      "\n",
      "Next Char Predictions:\n",
      " b' NxlnEVIAFNHPFtSvBnJuVriuzHKy\\xc3\\xb8LwSoS[UNK]tf\\xc3\\xb8IhbtkTh.b[UNK]oCP[UNK]uu.vx[UNK]d[UNK]Oib\\xc3\\xb8dggnEHk\\xc3\\xb8EOuxgdKoAaszFStFzkdFvvpsNCI'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c6c75ab8-c7af-4964-b245-0035313a79ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (2, 100, 53)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         3.96876\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1bd93d6-1881-4acc-aadc-f738345bb372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "173539b0-217b-4c55-a0c0-2e35b013e04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6dea3ecc-9a64-4a4b-a681-26f2b9f12117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "22/22 [==============================] - 16s 646ms/step - loss: 3.7537\n",
      "Epoch 2/20\n",
      "22/22 [==============================] - 14s 633ms/step - loss: 3.0089\n",
      "Epoch 3/20\n",
      "22/22 [==============================] - 14s 631ms/step - loss: 2.6928\n",
      "Epoch 4/20\n",
      "22/22 [==============================] - 14s 630ms/step - loss: 2.4601\n",
      "Epoch 5/20\n",
      "22/22 [==============================] - 14s 626ms/step - loss: 2.3203\n",
      "Epoch 6/20\n",
      "22/22 [==============================] - 14s 632ms/step - loss: 2.2086\n",
      "Epoch 7/20\n",
      "22/22 [==============================] - 14s 628ms/step - loss: 2.1041\n",
      "Epoch 8/20\n",
      "22/22 [==============================] - 14s 628ms/step - loss: 1.9979\n",
      "Epoch 9/20\n",
      "22/22 [==============================] - 14s 626ms/step - loss: 1.8873\n",
      "Epoch 10/20\n",
      "22/22 [==============================] - 14s 628ms/step - loss: 1.7553\n",
      "Epoch 11/20\n",
      "22/22 [==============================] - 14s 630ms/step - loss: 1.6391\n",
      "Epoch 12/20\n",
      "22/22 [==============================] - 14s 635ms/step - loss: 1.4774\n",
      "Epoch 13/20\n",
      "22/22 [==============================] - 14s 629ms/step - loss: 1.3227\n",
      "Epoch 14/20\n",
      "22/22 [==============================] - 14s 622ms/step - loss: 1.1459\n",
      "Epoch 15/20\n",
      "22/22 [==============================] - 14s 626ms/step - loss: 0.9514\n",
      "Epoch 16/20\n",
      "22/22 [==============================] - 14s 627ms/step - loss: 0.7595\n",
      "Epoch 17/20\n",
      "22/22 [==============================] - 14s 629ms/step - loss: 0.5677\n",
      "Epoch 18/20\n",
      "22/22 [==============================] - 13s 610ms/step - loss: 0.4318\n",
      "Epoch 19/20\n",
      "22/22 [==============================] - 13s 599ms/step - loss: 0.3758\n",
      "Epoch 20/20\n",
      "22/22 [==============================] - 13s 601ms/step - loss: 0.2849\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afadb668-4c13-4fce-8fb6-ac1c7cd544e8",
   "metadata": {},
   "source": [
    "## Predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea7357f1-5c2d-4196-8ee5-ec43faee57e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf at each bad index.\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model.\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "        predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                              return_state=True)\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits/self.temperature\n",
    "        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        # Return the characters and model state.\n",
    "        return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ed15a69f-6d31-43c6-8918-bb4505b53daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d011f1a6-4276-4899-8585-a7fd4bb12127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next(num_chars=300, start='NTNU'):\n",
    "    states = None\n",
    "    next_char = tf.constant([start])\n",
    "    result = [next_char]\n",
    "    \n",
    "    for n in range(num_chars):\n",
    "        next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "        result.append(next_char)\n",
    "\n",
    "    result = tf.strings.join(result)\n",
    "    print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "639f5584-c2fd-42c6-983a-5b1112e23a09",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_40044/1201141541.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredict_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_40044/2885928679.py\u001b[0m in \u001b[0;36mpredict_next\u001b[1;34m(num_chars, start)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_chars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'NTNU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mnext_char\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnext_char\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "predict_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65b2271-5513-40ee-9eed-3423a47ed483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
